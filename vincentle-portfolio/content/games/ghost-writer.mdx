---
title: "Ghost Writer"
roles: ["Solo Developer"]
hook: "Become the best within the Dead Poet's Society."
engine: "Unity"
platform: ["PC"]
duration: "2023â€”2024"
cover: "/images/games/ghost-writer.png"
sections: ["Overview", "Programming", "Links"]
status: "Prototype"
highlights:
- "Innovation in Q-learning algorithms for opponent AI"
tags: [unity, 2d, simulation]
relatedProjects:
    - { slug: "good-luck-valley", title: "Good Luck Valley", cover: "/images/games/good-luck-valley.jpg"}
    - { slug: "perennial", title: "Perennial", cover: "/images/games/perennial.png"}
---

## Overview
*Ghost Writer* is a 2D, top-down, writing-based management-tycoon game in which the player competes to become a bestseller,
within the Dead Poet's Society: a group of young, undead authors. Their goal is to stimulate the industry with
new ideas while keeping themselves financially afloat.

*Ghost Writer* was a prototype made in Unity that was meant to explore two things: modular software engineering principles
in game design (with its use of patterns such as the Mediator and Service Locator patterns), as well as innovation
within Q-learning algorithms for opponent AI. What resulted was a tycoon sim with built Q-Learning rivals that were
streamlined for staged design, who consistently crushed the random baselines and yielded clear tuning levers for difficulty.

## Programming
### Innovating Q-Learning for AI opponents
Most tycoon games ship with AI "rivals" that feel static and unchallenging, undercutting the simulation fantasy of a
competitive environment. I set out to build AI opponents for my writing-management game, *Ghost Writer*, that actually
learns to compete with the player over time.

I designed a modifier Q-Learning system tailored to *Ghost Writer*'s four chained development phases: Concept,
Focus 1, Focus 2, and Focus 3. However, during my research, I found that the original Q-Learning algorithm was
inadequate for this task, and I had to make some modifications to it. Instead of transitioning to a "next state"
each step, the learner references the current max Q-value, which reframes the discount term and removes the
"walk length" parameter, streamlining learning for staged workflows. I implemented the architecture in Unity
C#: ReinforcementProblem as the action evaluator, QValueStore/QLearner as modularized learning components,
and two brains - LearnedBrain for Q-Learning-based actions and RandomBrain for random actions. I evaluated
them across multi-trial datasets, scoring by average work quality and sales over three in-game years.

Across all trials, learned rivals significantly beat random agents. In 10-trial tests (3 iterations per step),
even the weakest learned setup outperformed the random agent by ~163% in sales. With deeper training (50 iterations per step),
top agents achieved ~99 average score (out of 100) and ~160k average sales vs. ~26k for random agents. This parameter
study showed the best rivals combine high learning and high exploration with a low discount factor.

I wrote a research paper diving into the details of the algorithm, its application in *Ghost Writer*, and the
game data derived from its use.

<a
    href="/pdfs/ghost-writer-research.pdf"
    target="_blank"
    rel="noopener noreferrer"
    className="underline underline-offset-4 hover:text-primary transition-colors"
>
    Read the full paper (PDF)
</a>

## Links
<a href="https://github.com/Le-Vincent56/Writer-Tycoon-Redux"
    target="_blank"
    rel="noopener noreferrer"
    className="underline underline-offset-4 hover:text-primary transition-colors"
>
    GitHub
</a>